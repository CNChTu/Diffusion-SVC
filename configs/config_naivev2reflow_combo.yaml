data:
  f0_extractor: 'fcpe' # 'parselmouth', 'dio', 'harvest', 'crepe', 'rmvpe' or 'fcpe'
  f0_min: 65 # about C2
  f0_max: 800 # about G5
  sampling_rate: 44100
  block_size: 512 # Equal to hop_length
  duration: 2 # Audio duration during training, must be less than the duration of the shortest audio clip
  encoder: 'contentvec768l12' # 'hubertsoft', 'hubertbase', 'hubertbase768', 'contentvec', 'contentvec768' or 'contentvec768l12' or 'cnhubertsoftfish'
  cnhubertsoft_gate: 10 # only use with cnhubertsoftfish
  encoder_sample_rate: 16000
  encoder_hop_size: 320
  encoder_out_channels: 768 # 256 if using 'hubertsoft'
  encoder_ckpt: pretrain/contentvec/checkpoint_best_legacy_500.pt
  units_forced_mode: 'nearest' # Recommended 'nearest',experiment 'rfa512to441' and 'rfa441to512' ; 'left'  only use for compatible with history code
  volume_noise: 0 # if not 0 ,add noise for volume in train ;;;;EXPERIMENTAL FUNCTION, NOT RECOMMENDED FOR USE;;;;
  train_path: op_data/train # Create a folder named "audio" under this path and put the audio clip in it
  valid_path: op_data/val # Create a folder named "audio" under this path and put the audio clip in it
  extensions: # List of extension included in the data collection
    - wav
model:
  t_start: 0.7 # 0.0 means reflow do anything, 1.0 means reflow do nothing
  type: 'ReFlow'
  n_hidden: 256
  use_pitch_aug: true  
  n_spk: 2 # max number of different speakers
  z_rate: 0 # dont change
  mean_only: true
  spec_min: -12
  spec_max: 2
  velocity_fn:
    type: 'LYNXNetDiff'
    cn_layers: 6
    cn_chans: 512
    use_mlp: false # is use MLP in cond_emb and output_proj
    mlp_factor: 4
    expansion_factor: 2
    kernel_size: 31
    conv_only: true # use Transformer block with conv block, if false
    wavenet_like: false # dont change if dont understand; more info:diffusion/naive_v2/naive_v2_diff.py
    use_norm: false # pre-norm for every layers
    conv_model_type: 'mode1'
    conv_dropout: 0.0
    atten_dropout: 0.1
    conv_model_activation: 'SiLU'
    GLU_type: 'GLU'
    channel_norm: false
  mask_cond_ratio: 'NOTUSE' # input 'NOTUSE' if not use
  loss_type: 'l2' # 'l1', 'l2' or 'l2_lognorm'
  naive_fn:
    type: 'LYNXNet' # LYNXNet is thr other name of ConformerNaiveEncoder(NaiveNet)
    n_layers: 3
    n_chans: 256
    simple_stack: false # use simple stack for unit emb
    out_put_norm: true # norm and weight_norm in last layer
    expansion_factor: 2
    kernel_size: 31
    conv_model_type: 'mode1' # dont change
    num_heads: 8
    use_norm: false # pre-norm for every layers
    conv_only: true # use Transformer block with conv block, if false
    conv_dropout: 0.0
    atten_dropout: 0.1
    use_weight_norm: false
  naive_fn_grad_not_by_reflow: false # dont change if dont understand; more info:diffusion/unit2mel.py
  naive_out_mel_cond_reflow: false # mel condition diffusion is a test function, maybe can make the model learn faster but less quality and pitch range.
  consistency: false
  consistency_only: true
  consistency_delta_t: 0.1
  consistency_lambda_f: 1.0
  consistency_lambda_v: 0.01 
device: 'cuda'
ddp:
  use_ddp: false #   if true, ddp_device will cover device and gpu id
  port: '13348'
  ddp_cache_gpu: false
  ddp_device:
    - 'cuda:1'
    - 'cuda:2'
    - 'cuda:3'
    - 'cuda:4'
vocoder:
  type: 'nsf-hifigan'
  ckpt: 'pretrain/nsf_hifigan/model'
infer:
  infer_step: 10
  method: 'euler' # 'euler', 'rk4'
env:
  expdir: exp/naivev2reflow
  gpu_id: 0
train:
  ema_decay: 0.999 # <1
  use_ema: false
  num_workers: 0 # If your cpu and gpu are both very strong, set to 0 may be faster!
  amp_dtype: fp16 # fp32, fp16 or bf16 (fp16 or bf16 may be faster if it is supported by your gpu)
  batch_size: 48
  cache_all_data: true # Save Internal-Memory or Graphics-Memory if it is false, but may be slow
  cache_device: 'cuda' # Set to 'cuda' to cache the data into the Graphics-Memory, fastest speed for strong gpu
  cache_fp16: false
  epochs: 100000
  interval_log: 100
  interval_val: 10000
  interval_force_save: 10000
  lr: 0.0002
  decay_step: 100000
  gamma: 0.5
  weight_decay: 0
  save_opt: false
